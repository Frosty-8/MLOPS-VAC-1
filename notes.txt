DATE : 6/02/25

- Airflow is the platform where you can process data from multiple data sources like RDBMS , NoSQL as of the machine 
  learning models do not allow multiple data sources to be accessed at one time so we require this platform.
- Apache Airflow is used for the scheduling and orchestration of data pipelines or workflows. Orchestration of data 
  pipelines refers to the sequencing, coordination, scheduling, and managing of complex data pipelines from diverse 
  sources.

PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS> python -m venv venv
PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS> .\venv\Scripts\activate

to activate the virtual env to install our dependencies that are required in developing models

pip install cookiecutter

search this in the browser searrch engine :- cookiecutter data science template +github

(venv) PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS> cookiecutter https://github.com/drivendataorg/cookiecutter-data-science -c v1
  [1/8] project_name (project_name): aiml
  [2/8] repo_name (aiml): aiml
  [3/8] author_name (Your name (or your organization/company/team)): aissms
  [4/8] description (A short description of the project.): this is related to mlops
  [5/8] Select open_source_license
    1 - MIT
    2 - BSD-3-Clause
    3 - No license file
    Choose from [1/2/3] (1): 1
  [6/8] s3_bucket ([OPTIONAL] your-bucket-for-syncing-data (do not include 's3://')): 
  [7/8] aws_profile (default): 
  [8/8] Select python_interpreter
    1 - python3
    2 - python
    Choose from [1/2] (1): 1


=============================================================================
*** DEPRECATION WARNING ***

Cookiecutter data science is moving to v2 soon, which will entail using
the command `ccds ...` rather than `cookiecutter ...`. The cookiecutter command
will continue to work, and this version of the template will still be available.
To use the legacy template, you will need to explicitly use `-c v1` to select it.

Please update any scripts/automation you have to append the `-c v1` option,
which is available now.

For example:
    cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science
=============================================================================

__init__.py is the entry point 

interpreter - executes the code line by line 


batch-processing is used to automate the real data 
spice park platform
its called in memory processing

Elasticnet to be used for adding the algorithms 

========================================================================================================================================================================

DATE : 07/02/2025

# The below commmand is used to open the mlflow ui to track our model

MLFLOW : MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible.

MLflow can be used in a variety of environments, including your local environment, on-premises clusters, cloud platforms, and managed services. Being an open-source platform, MLflow is vendor-neutral; no matter where you are doing machine learning, you have access to the MLflow’s core capabilities sets such as tracking, evaluation, observability, and more.



### this is the command for to launch the mlflow server to deploy models
(venv) PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS> mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts -host 127.0.0.1 -p 5000

(venv) PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS> mlflow ui 
 

Django : a framework based on MVT (Model View Template) design  platform . Django is a high-level Python web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.


from django.contrib import admin
from django.urls import path
from firstApp import views

urlpatterns = [
    path('admin/', admin.site.urls),
    path('',views.index)  
]
this tells us its not the good coding cause it creates Trojan Horse issue which means vulnerability
 


### This below code allows to acquire the required features and drop the target features from the model

from django.shortcuts import render
import joblib

def index(request):
    return render(request, 'index.html')

def result(request):
    cls = joblib.load('../models/model.joblib')
    
    # Get all query parameters
    features = request.GET.dict()
    
    # Remove the target feature if it exists
    target_feature = 'target'  # Replace 'target' with the actual target feature name if different
    if target_feature in features:
        del features[target_feature]
    
    # Collect feature values
    feature_values = list(features.values())

    # Predict using the model
    answer = cls.predict([feature_values])

    return render(request, 'index.html', {'answer': answer[0]})


####
(venv) PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS\webapp> python manage.py makemigrations
No changes detected
(venv) PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS\webapp> python manage.py makemigrations
Migrations for 'firstApp':
  firstApp\migrations\0001_initial.py
    + Create model mlops
(venv) PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS\webapp> python manage.py sqlmigrate firstApp 0001
BEGIN;
--
-- Create model mlops
--
CREATE TABLE "firstApp_mlops" ("id" bigint NOT NULL PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY, "age" integer NOT NULL, "sex" integer NOT NULL, "bmi" double precision NOT NULL, "children" integer NOT NULL, "smoker" integer NOT NULL, "region" integer NOT NULL, "charges" double precision NOT NULL);   
COMMIT;
(venv) PS C:\Users\Sarthak\OneDrive\Documents\Desktop\VAC-MLOPS\webapp> 


========================================================================================================================================================================

DATE : 08/02/2025


Docker is an engine which runs the container 
It is a container management service , keywords of Docker are developed , ship and run everywhere .
idea of Docker ship them into containers

docker build - containerize the images
docker pull - used to run your application
docker run - to run an image
Registry - where it contains pre images 

Docker images - an image is a combination of file system and parameters , everything is based in images 
              "docker run hello-world"
to use the CentOS image available in docker hub to run CentOS on our ubuntu machine
        sudo docker run -it centos /bin/bash

to list all the images in docker 
        docker images

TAG - used to logically tag image
Image ID - used to uniquely identify the image
Created - no . of days since the image was created
virtual size - size of the image

Container - its a container of images 

Docker volumes are widely used and useful tool for ensuring data persistence while working in containers

docker run -v /home/mount/data : /var/lib/mysql/data - >  to call the container

Docekr Networking 
it allows you to create a network of docker containers managed by a master node called the manager 
containers inside the docker network can talk each other by sharing packets
Bridge : if you build a container without specifying the kind of driver 
types of docker networking :-> none
                               host
commands :-> sudo docker network
             sudo docker network ls

ECR -> elastic container registry

docker security -> containers are immutable by nature , any modifications to a running container instance must first be made to the container image before beinng deployed 
1. keep host system and docker software updated
2. use official , verified docker images
3. run containers as non-root users
4. limit no.of open ports on a container - docker run -p 80:80 myimage
5. use encrypted environment variables and docker secrets
6. monitor the host and container logs - docker logs <container_name>
7. use network segmentation and firewalls : docekr networking , third - party network plugins , host & container - based firewalls
8. enable security features
9. regularly access and validate the security posture 
